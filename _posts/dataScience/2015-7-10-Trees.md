---
layout: post
title: Trees
category: DataScience
tags: [R, The_Analytics_Edge, edx]
---


### CART(classification and regression tree)
- parameter
	+ **minbucket**  
		minbucket↓ => over-fitting

### create CART model
```r
install.packages("rpart")
library(rpart)
install.packages("rpart.plot")
library(rpart.plot)
StevensTree = rpart(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, data=Train, method="class", minbucket=25)
prp(StevensTree)
PredictCART = predict(StevensTree, newdata=Test, type="class")
table(Test$Reverse, PredictCART) #can calc accuracy here
library(ROCR) #start generate ROC curve
PredictROC = predict(StevensTree, newdata=Test)
pred = prediction(PredictROC[,2], Test$Reverse)
perf = performance(pred, "tpr", "fpr")
plot(perf)
```
### random forest
- improve prediction accuracy of CART
	+ build many trees by using "bagged/bootstrapped"
	+ each trees "votes" on the outcome and we pick the outcome that receives the majority of the votes
- parameters
	+ **nodesize**: minimum number of observations in a subset  
		nodesize↓ => time↑
	+ **ntree**: number of trees  
		should not be too small 

### create random forest
```r
install.packages("randmForest")
library(randomForest)
set.seed(100)
StevensForest = randomForest(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, data=Train, nodesize=25, ntree=200)
# warning
Train$Reverse = as.factor(Train$Reverse)
Test$Reverse = as.factor(Test$Reverse)
StevensForest = randomForest(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, data=Train, nodesize=25, ntree=200)
PredictForest = predict(StevensForest, newdata=Test)
table(Test$Reverse, PredictForest)
```
### parameter selection
##### K-fold Cross-Validaition
![kobito.1435671718.939944.png](https://qiita-image-store.s3.amazonaws.com/0/44948/1e72a785-4373-4db0-25a0-978a6df6541a.png "kobito.1435671718.939944.png")

- output of K-fold
![kobito.1435671922.037141.png](https://qiita-image-store.s3.amazonaws.com/0/44948/c9100918-e71e-7b09-92fe-d58b9edfe5d3.png "kobito.1435671922.037141.png")

- parameters
	+ **CP(Complexity Parameter)**: like Adjusted $R^2$ and AIC  
		CP↓ => tree↑ (might overfit)

### using cross validation for CART model
```r
install.packages("caret")
library(caret)
install.packages("e1071")
library(e1071)
numFolds = trainControl(method="CV", number=10) #10 folds
cpGrid = expand.grid(.cp=seq(0.01,0.5,0.01))
#here we can get CP value
train(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, data=Train, method="rpart", trControl=numFolds, tuneGrid=cpGrid)
StevensTreeCV = rpart(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, data=Train, method="class", cp=0.18)
PredictCV = predict(StevensTreeCV, newdata=Test, type="class")
table(Test$Reverse, PredictCV)
```

### penalty error
- capture asymmetry  
	eg. failing to classify a **high-cost patient** correctly is **worse** than failing to classify a **low-cost patient** correctly  
![kobito.1435758804.780651.png](https://qiita-image-store.s3.amazonaws.com/0/44948/483011ff-adf9-acd1-49c6-0646a0a97766.png "kobito.1435758804.780651.png")

### calc penalty error & create new model with loss
```r
#create penalty error matrix
PenaltyMatrix = matrix(c(0,1,2,3,4,2,0,1,2,3,4,2,0,1,2,6,4,2,0,1,8,6,4,2,0), byrow=TRUE,nrow=5)
#cal the penalty error for baseline method
sum(as.matrix(table(ClaimsTest$bucket2009, ClaimsTest$bucket2008))*PenaltyMatrix) / nrow(ClaimsTest)
#create model with loss matrix
ClaimsTree = rpart(xxxxxxx, params=list, loss=PenaltyMatrix)
```

### regression trees
- Classification trees: report average outcome at each leaf of tree  
	$\frac{15}{15+5}=0.75$ >= 0.5(threshold) => TRUE
- regression trees: report average of the values  
	$3,4,5$ => 4







