---
layout: post
title: R Synax 
category: DataScience
tags: [R]
---

<!-- more -->

##basic
```r
getwd() #get working directory
a <- available.packages() #print the first 3 packages available
head(rownames(a), 3)
install.packages("knitr") #install package
install.packages(c("a", "b", "c"))
source("http://bioconductor.org/biocLite.R") #install R package from Bioconductor
biocLite()
library() #load packages
```

##read data
```r
read.table("URL")
load(url("URL"))
###########
file #opens a connection to a file
gzfile #opens a connection to a file compressed with gzip
bzfile #opens a connection to a file compressed with bzip2
url #opens a connection to a webpage
readLines(con, 10) #read each element one line at a time
```

##define data
```r
x <- c(0.5, "a") #create vectors (coercion character)
m <- matrix(nrow=2, ncol=3)
m <- matrix(1:6, nrow=2, ncol=3)
##########
m <- 1:10
dim(m) <- c(2, 5)
##########
x <- 1:3
y <- 10:12
cbind(x, y) #col bind
rbind(x, y) #row bind
##########
x <- list(1, "a", TRUE, 1 + 4i) #list
##########
x <- factor(c("yes", "yes", "no", "yes", "no"))
table(x)
unclass(x)
##########
x <- factor(c("yes", "yes", "no", "yes", "no"), levels=c("yes", "no"))
##########
is.na() #test objects if they are NA(missing value)
is.nan() #test for NAN (NaN is also NA, but the converse is not true)
##########
x <- data.frame(foo=1:4, bar=c(T, T, F, F))
##########
x <- 1:3 #about name
names(x) <- c("foo", "bar", "norf")
##########
x <- list(a=1, b=2, c=3)
##########
m <- matrix(1:4, nrow=2, ncol=2)
dimnames(m) <- list(c("a", "b"), c("c", "d"))
###Sequence###
seq(0, 10, by=0.5)
seq(5, 10, length=30)
rep(0, times=10)
rep(c(0,1,2), times=10)
rep(c(0,1,2), each=10)
###########
my_char <- c("My", "name", "is")
paste(my_char, collapse = " ") #"My name is"
my_name <- c(my_char, "Cindy")
paste(my_name, collapse = " ") #"My name is Cindy"
paste("Hello", "world!", sep=" ") #"Hello world!"
#LETTERS is a predefined variable in R containing a character vector of all 26 letters in the English alphabet
```

##access data
```r
data_set$variable #find the number of variable
data_set[111, 5] #row-and-column notation
data_set[1:10, 5] #create a range of values
data_set[1:10, ] #want all columns
dataset1 = data_set[!is.na(data_set$natspac), ]
dataset1 = subset(data_set, data_set$natspac != "NA")
dataset1 = na.omit(data_set) #clean the data
dataset2 = subset(data_set, select=c('age', 'natspac'))
dataset2 = subset(data_set, data_set$variable == "?") #return a data frame only contains ?
which(data_set$at_bats == 5579) #which observation in the dataset has 5,579 at bats using
which.max(data_set$sodium) #find which food has the highest sodium level
match("CAVIAR", data_set$Description) #dig the word CAVIAR in the Description vector
###########
x <- matrix(1:6, 2, 3)
x[1, 2, drop=FALSE] #return a 1*1 matrix
###########
x <- list(foo=1:4, bar=0.6, baz="hello")
x[c(1, 3)] #ONLY the 1st and 3rd elements of x
x[c(-1, -3)] #all elements of x EXCEPT for the 1st and 3rd elements
x <- list(aardvark=1:5)
x$a 
x[["a", exact=FALSE]]
x <- c(1,2,NA,4,NA,5) #remove NA values
bad <- is.na(x)
x[!bad]
x <- c(1,2,NA,4,NA,5)
y <- c("a","b",NA,"d",NA,"f")
good <- complete.cases(x,y) #good is the subset with no NA values
x[good]
y[good]
vect2 <- c(11, 2, NA) #add vect name
names(vect2) <- c("foo", "bar", "norf")
cnames <- c("patient", "age", "weight", "bp", "rating", "test")
colnames(my_data_frame) <- cnames #set column names of my_data_frame
```

##control structure
####for
```r
x <- c("a", "b", "c", "d")
for(i in 1:4)
for(i in seq_along(x))
for(letter in x)
x <- matrix(1:6, 2, 3)
for(i in seq_len(nrow(x))) {
	for(j in seq_len(ncol(x))) {
		print(x[i, j])
	}
}
```
####while
```r
count <- 0
while(count < 10) {}
```
####repeat
```r
repeat{}
```

##download files & read from APIs
```r
file.exists()
dir.create()
download.file(fileUrl, destfile="./xxx") #download file from internet. If the url starts with https, you need to set method="curl"
read.xlsx("./xxx.xlsx", sheetIndex=1, header=TRUE)
read.xlsx("./xxx.xlsx", sheetIndex=1, colIndex=2:3, rowIndex=1:4)
fread(fileName) #read file into R
####read XML####
doc <- xmlTreeParse(fileUrl, useInternal=TRUE)
rootNode <- xmlRoot(doc)
xmlName(rootNode) #root node of xml file
names(rootNode) #nodes of root
xmlSApply(rootNode, xmlValue) #specific contents of xml file
xpathSApply(rootNode, "//name", xmlValue) #/node->top level node, //node->node at any level, node[@attr-name], node[@attr-name='bob']
####extract content by attributes####
doc <- htmlTreeParse(fileUrl, useInternal=TRUE)
xpathSApply(doc, "//li[@class='class-name']", xmlValue)
####read from JSON####
jsonData <- fromJSON(url)
####write to JSON####
myjson <- toJSON(iris, pretty=TRUE)
cat(myjson)
####data.table####
library(data.table)
DT = data.table(x=rnorm(9), y=rep(c("a","b","c"),each=3), z=rnorm(9)) #create test data
DT[c(2, 3)] #subsetting rows
DT[, list(mean(x),sum(z))]
DT[, table(y)]
DT[, w:=z^2] #add new column
DT[, y:=2] #set column y to 2
DT <- data.table(x=rep(c("a","b","c"), each=100), y=rnorm(300))
setkey(DT, x) #x is the key
DT['a'] #pick up rows of a
####join####
setkey(DT1, x); setkey(DT2, x)
merge(DT1, DT2)
####MySQL####
install.packages("RMySQL")
library(RMySQL)
ucscDb <- dbConnect(MySQL(), user="genome", host="genome-mysql.cse.ucsc.edu")
dbDisconnect(ucscDb)
result <- dbGetQuery(ucscDb, "show databases;")
db_name <- dbConnect(MySQL(), user="genome", db="db_name", host="genome-mysql.cse.ucsc.edu")
allTables <- dbListTables(db_name)
dbListFields(db_name, "table_name")
dbGetQuery(db_name, "select count(*) from affyU133Plus2")
data <- dbReadTable(db_name, "table_name")
head(data)
query <- dbSendQuery(db_name, "select * from ...")
data <- fetch(query)
data <- fetch(query, n=10)
quantile(data$field_name)
quantile(data$field_name, probs=c(0.5, 0.75, 0.9))
dbClearResult(query) #don't forget to close the connection
dbDisconnect(db_name)
####HDF5####
source("http://bioconductor.org/biocLite.R")
biocLite("rhdf5")
library(rhdf5)
created = h5createFile("example.h5")
created = h5createGroup("example.h5", "foo")
A = matrix(1:10, nr=5, nc=2) #write to groups
h5write(A, "example.h5", "foo/A")
df = data.frame(1L:5L, seq(0,1,length.out=5), c("ab","cde","fghi","a","s"), stringsAsFactors=FALSE)
h5write(df, "example.h5", "df") #write a data set
readA = h5read("example.h5", "foo/A") #read data
h5write(c(12,13,14), "example.h5", "foo/A", index=list(1:3,1)) #writing & reading chunks
head("example.h5", "foo/A")
h5ls("example.h5") #structure of example.h5
####Webscraping####
con = url("url") #get data off webpages
htmlCode = readLines(con)
close(con)
library(XML) #parsing with xml
url <- ""
html <- htmlTreeParse(url, useInternalNodes=T)
xpathSApply(html, "//td[@id='col-citedby']", xmlValue)
library(httr) #GET from the httr package
html2 = GET(url, authenticate("user","passwd"))
content2 = content(html2, as="text")
parsedHtml = htmlParse(content2, asText=TRUE)
xpathSApply(parsedHtml, "//title", xmlValue)
google = handle("http://google.com") #use handlesß
pg1 = GET(handle=google,path="/")
pg2 = GET(handle=google,path="search")
####read from APIs####
myapp = oauth_app("twitter", key="yourConsumerKeyHere", secret="yourConsumerSecretHere")
sig = sign_oauth1.0(myapp, token="yourTokenHere", token_secret="yourTokenSecretHere")
homeTL = GET("url/xx.json", sig)
json1 = content(homeTL)
json2 = jsonlite::fromJSON(toJSON(json1)) #convert from JSON to data frame
####sqldf####
download.packages("sqldf")
library(sqldf) #sqldf packages allows for execution of SQL commands on R data frame
```

##get known of data
```r
str(data_set) #data structure
summary(data_set$variable) #return a numerical summary: minimun, first quartile, median, mean, thid quartile, and maximun
dim(data_set) #the number of rows and variables with the dim function
names(data_set) #names of the variables of the data frame
head(data_set) #first few entries
tail(data_set) #last few entries
mean(data_set$variable) #average
var(data_set$variable) #variance
median(data_set$variable) #median
sd(data_set$sodium, na.rm=TRUE) #standard deviation
nrow(dataset) # count row number
#get known about data
table(data_set$variable) #counts the number of times each kind of category occurs in a variable
table(CPS$Region, is.na(CPS$Married))
by(weight, habit, mean) #split the weight variable into the habit groups, then take the mean of each
by(numerical_dataset, categorical_dataset, length) #the sizes of the respective groups
tapply(USDA$Iron, USDA$HighProtein, mean, na.rm=TRUE) #Group Iron by Protein, apply the mean function
tapply(CocaCola$StockPrice, months(CocaCola$Date), mean, na.rm=TRUE) #Group by sort months, apply the mean function
tapply(limited$Info.On.Internet, limited$Smartphone, summary, na.rm=TRUE) #Group by Smartphone, apply the summary function
sort(tapply(is.na(CPS$MetroAreaCode), CPS$State, mean))
sort(tapply(CPS$Race == "Asian", CPS$MetroArea, mean))
sort(table(mvt$LocationDescription))
table(data_set$field_name %in% c("21212"))
data_set[data_set$field_name %in% c("", ""),]
#Subsetting
set.seed(13435)
X <- data.frame("var1"=sample(1:5), "var2"=sample(6:10), "var3"=sample(11:15))
X$var2[c(1, 3)] = NA
X[which(X$var2 > 8),] #dealing with missing values
sort(X$var1, decreasing=TRUE)
sort(X$var2, na.last=TRUE) #6 7 10 NA NA  na.last=FALSE-> NA NA 6 7 10
X[order(X$var1),]
#ordering with plyr
library(plyr)
arrange(X, var1)
arrange(X, desc(var1))
X$var4 <- rnorn(5) #add column
Y <- cbind(X, rnorm(5)) #add column
#cross tabs
xtabs(Freq ~ Y + X, data=data_set)
ftable(xt) #convert to flat table
object.size(data) #size of a data set
print(object.size(data), units="Mb")
#creat categorical variables
cut(data_set$field_name, breaks=quantile(data_set$field_name)) 
#easier cutting
library(Hmisc)
cut2(data_set$field_name, g=4) #same result with above
#levels of factor variables
yesno <- sample(c("yes", "no"), size=10, replace=TRUE)
yesnofac = factor(yesno, levels=c("yes", "no"))
relevel(yesnofac, ref="yes")
#use mutate to transform the data
library(Hmisc); library(plyr);
mutate(data_set, data_set$field_name=cut2(zipCode, g=4))
#common transform
ceiling(3.475) = 4
floor(3.475) = 3
round(3.475, digits=2) #3.48
signif(3.475, digits=2) #3.5
exp(x) #exponentiating
#reshaping
library(reshape2)
#melt data frames
mtcars$carname <- rownames(mtcars)
carMelt <- melt(mtcars, id=c("carname", "gear", "cyl"), measure.vars=c("mpg", "hp"))
head(carMelt, n=3) #result: colname-> carname gear cyl variable value (variable is "mpg" and "hp")
#cast data frames
dcast(carMelt, cyl ~ variable) #colname-> cyl mpg hp
dcast(carMelt, cyl ~ variable,mean) #colname-> cyl mpg hp (mean of mpg and hp)
#split
spIns = split(data_set$field_name1, data_set$field_name2) 
sprCount = lapply(spIns, sum) #list, result is the same with tapply(xx,xx,sum)
unlist(sprCount) #combine
sapply(spIns, sum)
#merge table
#####################
#The first two arguments determine the data frames to be merged 
#(they are called "x" and "y", respectively, in the subsequent parameters to the merge function). 
#by.x="MetroAreaCode" means we're matching on the MetroAreaCode variable from the "x" data frame (CPS), 
#while by.y="Code" means we're matching on the Code variable from the "y" data frame (MetroAreaMap). 
#Finally, all.x=TRUE means we want to keep all rows from the "x" data frame (CPS), even if some of the rows' MetroAreaCode doesn't match any codes in MetroAreaMap 
#(for those familiar with database terminology, this parameter makes the operation a left outer join instead of an inner join).
#####################
CPS = merge(CPS, MetroAreaMap, by.x="MetroAreaCode", by.y="Code", all.x=TRUE)
#select columns of the same name
intersect(names(data_set1), names(data_set2))
#use join in the plyr package
arrange(join(data_set1, data_set2), id)
#join multiple data frames
dfList = list(df1, df2, df3)
join_all(dfList)
```

##date
```r
USDA$HighSodium = as.numeric(USDA$sodium > mean(USDA$sodium, na.rm=TRUE)) #change the data type to numeric + add HighSodium variable
DateConvert = as.Date(strptime(mvt$Date, "%m/%d/%y %H:%M"))
mvt$Month = months(DateConvert)
mvt$Weekday = weekdays(DateConvert)
mvt$Date = DateConvert
POSIXct #a large Integer
POSIXlt #store a bunch of other useful information
x <- as.POSIXlt(x)
d1 = date() #class(d1): character
d2 = Sys.Date() #class(d2): Date
format(d2, "%a %b %d")
x = c("1jan1960"); z = as.Date(x, "%d %b %y")
library(lubridate)
ymd("20140108") #"2014-01-08 UTC"
mdy("08/04/2013") #"2013-08-04"
dmy("03-04-2013") #"2013-04-03"
ymd_hms("2011-08-03 10:15:03", tz = "Pacific/Auckland")
?Sys.timezone
```

##edit text variables
```r
tolower()
toupper()
strsplit("a.a", "\\.") #return list "a" "a"
#fix character
sub("_", "", c("id", "sub_id_id"),) #return c("id", "subid_id")
gsub("_", "", c("id", "sub_id_id")) #return c("id", "subidid")
#find character
grep("_", c("id", "sub_id")) #return 2
grep("_", c("id", "sub_id"), value=TRUE) #return sub_id
grepl("_", c("id", "sub_id")) #FALSE TRUE
#length of string
nchar("abc") #return 3
substr("abc", 1, 2) #return ab
str_trim("a  ") #return a
```

##regular expressions
```r
^i think #match all sentances with "i think" start
moring$ #match all lines ends with moring
[Bb][Uu][Ss][Hh] #Bush bush...
^[0-9][a-zA-Z] #7th 2nd...
. #refer any character
flood|fire #refer flood or fire
* #any number, including none, of the item
+ #at least one of the item
```

##about apply
```r
x <- list(a=1:5, b=rnorm(10))
lapply(x, mean) #return a list of mean
sapply() #simplify the result of lapply
apply(X, MARGIN, FUN) #MARGIN: for a matrix, 1 indicates rows, 2 indicates columns, c(1, 2) indicates rows and columns
rowSums = apply(x, 1, sum)
rowMeans = apply(x, 1, mean)
colSums = apply(x, 2, sum)
colMeans = apply(x, 2, mean)
apply(x, 1, quantile, probs=c(0.25, 0.75))
a <- array(rnorm(2*2*10), c(2,2,10))
apply(a, c(1,2), mean) = rowMeans(a, dims=2)
mapply(rep, 1:4, 4:1) #result: list(rep(1,4), rep(2,3), rep(3,2), rep(4,1))
```

##debug
```r
traceback()
debug(lm)
lm(y ~ x) #use c to quit
options(error = recover)
```

##calculate
```r
#####################
#z score
#####################
pnorm() #cal percentile
qnorm() #cal observed value / cutoff value
#####################
#t score
#####################
pt(T score, df) # 2*pt(T score, df, lower.tail=FALSE)p-value
qt(p value, df) #critical t score
#####################
#f
#####################
pf(F score, dfg, dfe, lower.tail=FALSE) #p value
#####################
#binomial
#####################
sum(dbinom(a:b, n, proportion))
system.time(replicate(500,mean(DT$pwgtp15,by=DT$SEX)))
```

##plot
```r
plot(x, y, type="") #type:line... 
plot(y ~x)
plot(x, y, xlab="x", ylab="y", main="title", col="red")
boxplot(data_set$variable)
boxplot(y ~ x) #compare the heights of men and women
hist(data_set$variable, breaks = 50) #50->bins
hist(x, xlab="x", main="title", xlim=c(0, 100), breaks=100)
rug(x)
barplot(table())
mosaicplot(table(gender, smoke100)) #how many people smoke across each gender
mosaicplot(evals$rank ~ evals$gender)
with(pollution, plot(x, y)) #scatterplot
abline(h = 12, lwd = 2, lty = 2) #lty: line type
#current active graphics device
dev.cur()
library(datasets)
airquality <- transform(airquality, Month = factor(Month))
boxplot(Ozone ~ Month, airquality)
with(airquality, plot(Wind, Ozone, main="O..."))
with(subset(airquality, Month==5), points(Winds, Ozone, col="blue"))
legend("topright", pch=1, col=c("blue","red"), legend=c("May", "Other"))
#multiple base plots
par(mfrow=c(1,2))
with(airquality, {
	plot(x, y)
	plot(x, y)
	mtext("title of all plots", outer = TRUE)
	})
#####################
# an example for plot
# The first step is to make a vector p that is a sequence from 0 to 1 with each number separated by 0.01
# We then create a vector of the margin of error (me) associated with each of these values of p using the familiar approximate formula (ME = 2 X SE)
# Finally, plot the two vectors against each other to reveal their relationship
#####################
n = 1000
p = seq(0, 1, 0.01)
me = 2 * sqrt(p * (1 - p)/n)
plot(me ~ p)
```

##Probability & sampling
```r
outcomes = c("heads", "tails") # samples
sample(outcomes, size=1, replace=true, prob=c(0.2,0.8)) #The replace = TRUE argument indicates we put the chip back in the hat before drawing again, therefore always keeping the 50/50 odds.
choose(9, 2)
sample(data_set, 50) #take a random sample from the vector
#####################
# an example for loop
# Set up an empty vector or 5000 NAs to store sample means
# Take 5000 samples of size 50 of 'area' and store all of them in 'sample_means50'.
# View the result. If you want, you can increase the bin width to show more
# detail by changing the 'breaks' argument.
#####################
sample_means50 = rep(NA, 5000) 
for (i in 1:5000) {
    samp = sample(area, 50)
    sample_means50[i] = mean(samp)
}
hist(sample_means50, breaks = 13)
#####################
# an example for par
# Divide the plot in 3 rows:
# Define the limits for the x-axis:
# Draw the histograms:
#####################
par(mfrow = c(3, 1))
xlimits = range(sample_means50)
hist(sample_means50, breaks = 20, xlim = xlimits)
```

##inference & ht
```r
se = sd(samp)/sqrt(60) # Calculate the standard error:
lower = sample_mean - 1.96 * se # Calculate the lower and upper bounds of your confidence interval and print them
upper = sample_mean + 1.96 * se
c(lower, upper)

source("http://bit.ly/dasi_inference")
#####################
# inference for bootstrap
# perc->percentile se->standard error
#####################
inference(nc$gained, type = "ci", method = "simulation", conflevel = 0.9, est = "mean", boot_method = "perc") 
#####################
# inference general
#The first argument is y, which is the response variable that we are interested in: nc$weight.
#The second argument is the grouping variable, x, which is the explanatory variable – the grouping variable accross the levels of which we’re comparing the average value for the response variable, smokers and non-smokers: nc$habit.
#The third argument, est, is the parameter we’re interested in: "mean" (other options are "median", or "proportion".)
#Next we decide on the type of inference we want: a hypothesis test ("ht") or a confidence interval("ci").
#When performing a hypothesis test, we also need to supply the null value, which in this case is 0, since the null hypothesis sets the two population means equal to each other.
#The alternative hypothesis can be "less", "greater", or "twosided".
#Lastly, the method of inference can be "theoretical" or "simulation" based.
#####################
inference(y = nc$weight, x = nc$habit, est = "mean", type = "ht", null = 0, alternative = "twosided", method = "theoretical", order = c("first", "second"))
#####################
# inference for ANOVA
#####################
inference(y = gss$wordsum, x = gss$class, est = "mean", method = "theoretical", type = "ht", alternative = "greater")
#####################
# inference for proportion
#####################
inference(us12$response, est = "proportion", type = "ci", method = "theoretical", success = "atheist")
```

##model & other related
```r
qqnorm(income[gender == "male"])
qqline(income[gender == "male"])
qqnorm(income[gender == "female"])
qqline(income[gender == "female"])
cor(input1, input2) #calculate the correlation
model = lm(formula, data = your_dataframe)  #Create a linear model
m1 = lm(runs ~ at_bats, data = mlb11)
m1 = lm(runs ~ ., data = mlb11) #using all the variables
summary(m1)
m1$residules #look at the residules
SSE = sum(model$residuals^2)
resid(model) #List of residuals
predict(model, newdata = newData) # predict new data
step(model) #automate to simplify the model
plot(density(resid(model))) #A density plot
qqnorm(resid(model)) or qqnorm(model$residuals)#A quantile normal plot - good for checking normality
qqline(resid(model)) or qqline(model$residuals)
plot(m_bty$residuals ~ m_bty$fitted) #residuals plot
hist(model$residuals)
abline(m1) #plots a line based on its slope and intercept
plot_ss(x = mlb11$at_bats, y = mlb11$runs, showSquares = TRUE, leastSquare = TRUE) #Adapt the function to plot the best fitting line
m_bty_gen = lm(evals$score ~ evals$bty_avg + evals$gender)
multiLines(m_bty_gen)
abline(v=as.Date(c("2000-03-01")), lwd=2) #draw a vertical line when date=2000-03-01
#select unordered variable as reference level
data$column = relevel(data$column, "ReferenceLevel")
```

##package
```r
###zoo
lag(zoo(data$column), -2, na.pad=TRUE) #-2 passed to lag means to return 2 observations before the current one;na.pad=TRUE means to add missing values for the first two values
###caTools
set.seed(144) #set the seed and then split twice, different splits
split=sample.split(data$column, SplitRatio=0.7)
train=subset(data, split=TRUE)
test=subsets(data, split=FALSE)
```

##?
```r
testFolder <- dir('UCI HAR Dataset/test')[-1]
trainFolder <- dir('UCI HAR Dataset/train')[-1]
```




